{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiheader Classification Beam Damage Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation  & Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-25T13:30:44.612585Z",
     "iopub.status.busy": "2025-05-25T13:30:44.611871Z",
     "iopub.status.idle": "2025-05-25T13:30:45.512081Z",
     "shell.execute_reply": "2025-05-25T13:30:45.511418Z",
     "shell.execute_reply.started": "2025-05-25T13:30:44.612557Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Loading dataset...\n",
      "✅ Loaded 452 samples\n",
      "🧪 Split: 361 train / 91 val\n",
      "📊 Original label distribution: {1: 60, 3: 15, 8: 124, 7: 48, 9: 35, 6: 103, 0: 96, 4: 56, 10: 3, 5: 4}\n",
      "🔍 Rare labels (<50 occurrences): [3, 7, 9, 10, 5]\n",
      "🔁 Oversampling label 3: duplicating 15 rows x6\n",
      "🔁 Oversampling label 7: duplicating 48 rows x1\n",
      "🔁 Oversampling label 9: duplicating 35 rows x1\n",
      "🔁 Oversampling label 10: duplicating 3 rows x35\n",
      "🔁 Oversampling label 5: duplicating 4 rows x27\n",
      "✅ Saved oversampled training set → oversampled_train.csv\n",
      "✅ Saved validation set → validation_set.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "# === CONFIG ===\n",
    "original_csv = \"Final-Competition-2025/cleaned_renamed_labeled_dataset.csv\"\n",
    "output_train_csv = \"oversampled_train.csv\"\n",
    "output_val_csv = \"validation_set.csv\"\n",
    "\n",
    "# === STEP 1: Load dataset ===\n",
    "print(\"📥 Loading dataset...\")\n",
    "df = pd.read_csv(original_csv)\n",
    "print(f\"✅ Loaded {len(df)} samples\")\n",
    "\n",
    "# === STEP 2: Split into train and val ===\n",
    "train_df, val_df = train_test_split(\n",
    "    df, test_size=0.2, stratify=df[\"class_label\"], random_state=42\n",
    ")\n",
    "print(f\"🧪 Split: {len(train_df)} train / {len(val_df)} val\")\n",
    "\n",
    "# === STEP 3: Oversample rare damage labels ===\n",
    "train_df = train_df.copy()\n",
    "train_df[\"damage_labels\"] = train_df[\"damage_labels\"].fillna(\"\").astype(str).str.split(\",\")\n",
    "\n",
    "# Count all labels\n",
    "flat_labels = [int(lbl) for sub in train_df[\"damage_labels\"] for lbl in sub if lbl.strip().isdigit()]\n",
    "label_counts = Counter(flat_labels)\n",
    "print(\"📊 Original label distribution:\", dict(label_counts))\n",
    "\n",
    "# Define rare labels\n",
    "rare_labels = [label for label, count in label_counts.items() if count < 50]\n",
    "print(f\"🔍 Rare labels (<50 occurrences): {rare_labels}\")\n",
    "\n",
    "# Oversample each rare label based on its scarcity\n",
    "augmented = [train_df]\n",
    "for label in rare_labels:\n",
    "    matching_rows = train_df[train_df[\"damage_labels\"].apply(lambda lst: str(label) in lst)]\n",
    "    n_repeat = max(1, 3 * (50 - label_counts[label]) // (len(matching_rows) + 1))\n",
    "    print(f\"🔁 Oversampling label {label}: duplicating {len(matching_rows)} rows x{n_repeat}\")\n",
    "    augmented.append(pd.concat([matching_rows] * n_repeat))\n",
    "\n",
    "augmented_train_df = pd.concat(augmented, ignore_index=True)\n",
    "\n",
    "# Convert list back to string\n",
    "augmented_train_df[\"damage_labels\"] = augmented_train_df[\"damage_labels\"].apply(lambda x: \",\".join(x))\n",
    "\n",
    "# === STEP 4: Save train and validation sets ===\n",
    "val_df = val_df.copy()\n",
    "val_df[\"damage_labels\"] = val_df[\"damage_labels\"].fillna(\"\").astype(str)\n",
    "\n",
    "augmented_train_df.to_csv(output_train_csv, index=False)\n",
    "val_df.to_csv(output_val_csv, index=False)\n",
    "\n",
    "print(f\"✅ Saved oversampled training set → {output_train_csv}\")\n",
    "print(f\"✅ Saved validation set → {output_val_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🖥️ System: Darwin arm64\n",
      "🚀 Using Apple Silicon GPU via MPS\n",
      "📊 PyTorch Version: 2.7.0\n"
     ]
    }
   ],
   "source": [
    "# Import the helper module\n",
    "from m3pro_gpu_helper import setup_m3pro_gpu, seed_everything\n",
    "\n",
    "# Replace your device setup with this line\n",
    "device = setup_m3pro_gpu()\n",
    "\n",
    "# Replace your current seeding function with this\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Multi-Header Classification Model for Beam Damage Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T13:31:00.941775Z",
     "iopub.status.busy": "2025-05-25T13:31:00.941510Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeanviaunelvictor/Documents/NTU-CAE/2025-spring/DeepLearningInComputerVision-2025/Final-Projects/final-Beam/.venv/lib/python3.10/site-packages/albumentations/core/validation.py:111: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n",
      "Epoch 1/50:   9%|▉         | 2/22 [05:07<51:13, 153.67s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 151\u001b[0m\n\u001b[1;32m    149\u001b[0m class_outputs, damage_outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[1;32m    150\u001b[0m loss \u001b[38;5;241m=\u001b[39m ce_loss(class_outputs, class_labels) \u001b[38;5;241m+\u001b[39m bce_loss(damage_outputs, damage_labels)\n\u001b[0;32m--> 151\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    153\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/Documents/NTU-CAE/2025-spring/DeepLearningInComputerVision-2025/Final-Projects/final-Beam/.venv/lib/python3.10/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/NTU-CAE/2025-spring/DeepLearningInComputerVision-2025/Final-Projects/final-Beam/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/NTU-CAE/2025-spring/DeepLearningInComputerVision-2025/Final-Projects/final-Beam/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torchvision import models\n",
    "\n",
    "# # === SEED EVERYTHING FOR REPRODUCIBILITY ===\n",
    "# def set_seed(seed=42):\n",
    "#     random.seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "#     torch.manual_seed(seed)\n",
    "#     torch.cuda.manual_seed(seed)\n",
    "#     torch.cuda.manual_seed_all(seed)\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "#     torch.backends.cudnn.benchmark = False\n",
    "#     os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# set_seed(42)\n",
    "\n",
    "# === Albumentations Dataset ===\n",
    "class AlbumentationsDamageDataset(Dataset):\n",
    "    def __init__(self, csv_path, root_dir, transform=None):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.class_folders = {18: \"Class A\", 19: \"Class B\", 20: \"Class C\"}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        class_label = int(row[\"class_label\"])\n",
    "        subfolder = self.class_folders[class_label]\n",
    "        img_path = os.path.join(self.root_dir, subfolder, row[\"id\"])\n",
    "\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image=image)[\"image\"]\n",
    "\n",
    "        class_tensor = torch.tensor(class_label - 18, dtype=torch.long)\n",
    "\n",
    "        damage_vector = torch.zeros(11)\n",
    "        if pd.notna(row[\"damage_labels\"]):\n",
    "            for l in str(row[\"damage_labels\"]).split(\",\"):\n",
    "                if l.strip().isdigit():\n",
    "                    damage_vector[int(l)] = 1.0\n",
    "\n",
    "        return image, class_tensor, damage_vector\n",
    "\n",
    "\n",
    "class MultiTaskDamageModel(nn.Module):\n",
    "    def __init__(self, version='b4', num_classes=3, num_damage_labels=11):\n",
    "        super().__init__()\n",
    "        \n",
    "        if version == 'b3':\n",
    "            base = models.efficientnet_b3(weights=\"IMAGENET1K_V1\")\n",
    "        elif version == 'b4':\n",
    "            base = models.efficientnet_b4(weights=\"IMAGENET1K_V1\")\n",
    "        elif version == 'b5':\n",
    "            base = models.efficientnet_b5(weights=\"IMAGENET1K_V1\")\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported EfficientNet version: {version}\")\n",
    "\n",
    "        in_features = base.classifier[1].in_features\n",
    "        base.classifier = nn.Identity()\n",
    "        \n",
    "        self.backbone = base\n",
    "        self.class_head = nn.Sequential(nn.Dropout(0.4), nn.Linear(in_features, num_classes))\n",
    "        self.damage_head = nn.Sequential(nn.Dropout(0.4), nn.Linear(in_features, num_damage_labels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        class_logits = self.class_head(features)\n",
    "        damage_logits = self.damage_head(features)\n",
    "        return class_logits, damage_logits\n",
    "\n",
    "\n",
    "# === Transformations ===\n",
    "train_transform = A.Compose([\n",
    "    A.RandomResizedCrop(size=(300, 300), scale=(0.8, 1.0), ratio=(0.9, 1.1), p=1.0),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.3),\n",
    "    A.Normalize(),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(height=300, width=300),\n",
    "    A.Normalize(),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "\n",
    "# === Config ===\n",
    "csv_path = \"oversampled_train.csv\"\n",
    "image_root = \"Final-Competition-2025\"\n",
    "num_epochs = 50\n",
    "batch_size = 32\n",
    "learning_rate = 1e-4\n",
    "val_split = 0.1\n",
    "threshold = 0.5 # with 0.4 we got 64 % model:version='b4',batch=32, \n",
    "num_damage_labels = 11\n",
    "damage_ignore_label = 2\n",
    "\n",
    "# === Dataset & Dataloaders ===\n",
    "full_dataset = AlbumentationsDamageDataset(csv_path, image_root, transform=train_transform)\n",
    "val_size = int(len(full_dataset) * val_split)\n",
    "train_size = len(full_dataset) - val_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "val_dataset.dataset.transform = val_transform\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# === Model, Losses, Optimizer ===\n",
    "model = MultiTaskDamageModel().to(device)\n",
    "ce_loss = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "rare_labels = [1, 3, 4]\n",
    "pos_weights = torch.tensor([5.0 if i in rare_labels else 1.0 for i in range(num_damage_labels)])\n",
    "pos_weights[damage_ignore_label] = 0.0\n",
    "bce_loss = nn.BCEWithLogitsLoss(pos_weight=pos_weights.to(device))\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# === Training Loop ===\n",
    "best_val_damage_f1 = 0.0\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    class_preds_train, class_targets_train = [], []\n",
    "    damage_preds_train, damage_targets_train = [], []\n",
    "\n",
    "    for images, class_labels, damage_labels in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs}\"):\n",
    "        images, class_labels, damage_labels = images.to(device), class_labels.to(device), damage_labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        class_outputs, damage_outputs = model(images)\n",
    "        loss = ce_loss(class_outputs, class_labels) + bce_loss(damage_outputs, damage_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        class_preds = class_outputs.argmax(dim=1)\n",
    "        damage_probs = torch.sigmoid(damage_outputs)\n",
    "        damage_bin = (damage_probs > threshold).float()\n",
    "        class_preds_train.extend(class_preds.cpu().numpy())\n",
    "        class_targets_train.extend(class_labels.cpu().numpy())\n",
    "        damage_preds_train.append(damage_bin.cpu())\n",
    "        damage_targets_train.append(damage_labels.cpu())\n",
    "\n",
    "    # === Validation ===\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    class_preds_val, class_targets_val = [], []\n",
    "    damage_preds_val, damage_targets_val = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, class_labels, damage_labels in val_loader:\n",
    "            images, class_labels, damage_labels = images.to(device), class_labels.to(device), damage_labels.to(device)\n",
    "            class_outputs, damage_outputs = model(images)\n",
    "            loss = ce_loss(class_outputs, class_labels) + bce_loss(damage_outputs, damage_labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            class_preds = class_outputs.argmax(dim=1)\n",
    "            class_preds_val.extend(class_preds.cpu().numpy())\n",
    "            class_targets_val.extend(class_labels.cpu().numpy())\n",
    "\n",
    "            damage_probs = torch.sigmoid(damage_outputs)\n",
    "            damage_bin = (damage_probs > threshold).float()\n",
    "            for i in range(damage_bin.size(0)):\n",
    "                if damage_bin[i].sum() == 0:\n",
    "                    top_index = damage_probs[i].topk(1).indices.item()\n",
    "                    if top_index != damage_ignore_label:\n",
    "                        damage_bin[i, top_index] = 1.0\n",
    "            damage_preds_val.append(damage_bin.cpu())\n",
    "            damage_targets_val.append(damage_labels.cpu())\n",
    "\n",
    "    # === Metrics ===\n",
    "    damage_preds_train = torch.cat(damage_preds_train).numpy()\n",
    "    damage_targets_train = torch.cat(damage_targets_train).numpy()\n",
    "    damage_preds_val = torch.cat(damage_preds_val).numpy()\n",
    "    damage_targets_val = torch.cat(damage_targets_val).numpy()\n",
    "\n",
    "    train_acc = accuracy_score(class_targets_train, class_preds_train)\n",
    "    val_acc = accuracy_score(class_targets_val, class_preds_val)\n",
    "    train_f1 = f1_score(class_targets_train, class_preds_train, average=\"macro\")\n",
    "    val_f1 = f1_score(class_targets_val, class_preds_val, average=\"macro\")\n",
    "    train_damage_f1 = f1_score(damage_targets_train, damage_preds_train, average=\"macro\", zero_division=0)\n",
    "    val_damage_f1 = f1_score(damage_targets_val, damage_preds_val, average=\"macro\", zero_division=0)\n",
    "\n",
    "    print(f\"\\n📊 Epoch {epoch} Summary:\")\n",
    "    print(f\"Train Loss: {total_loss:.4f} | Train Acc: {train_acc:.4f} | Train F1: {train_f1:.4f} | Damage F1: {train_damage_f1:.4f}\")\n",
    "    print(f\" Val Loss: {val_loss:.4f} | Val Acc:   {val_acc:.4f} | Val F1:   {val_f1:.4f} | Damage F1: {val_damage_f1:.4f}\")\n",
    "\n",
    "    if val_damage_f1 > best_val_damage_f1:\n",
    "        best_val_damage_f1 = val_damage_f1\n",
    "        torch.save(model.state_dict(), \"best_model_by_val_damage_f1.pth\")\n",
    "        print(\"💾 Best model updated!\")\n",
    "\n",
    "torch.save(model.state_dict(), \"multitask_model.pth\")\n",
    "print(\"\\n✅ Training complete. Final model saved.\")\n",
    "print(f\"🏆 Best model F1 (damage): {best_val_damage_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from model import MultiTaskDamageModel\n",
    "\n",
    "# === CONFIG ===\n",
    "test_dir = \"test_data/beam\"\n",
    "model_path = \"multitask_model.pth\"\n",
    "thresholds = [0.3, 0.35,0.5, 0.75, 0.8]  # Only run best\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === Allowed damage labels by class ===\n",
    "allowed_damage = {\n",
    "    18: [0],\n",
    "    19: [3, 4, 6, 8],\n",
    "    20: [1, 5, 7, 9, 10]\n",
    "}\n",
    "\n",
    "# === Load model ===\n",
    "model = MultiTaskDamageModel().to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# === Transform ===\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# === Run threshold sweep (only 0.35 here) ===\n",
    "for threshold in thresholds:\n",
    "    results = []\n",
    "    label_counts = []\n",
    "\n",
    "    for fname in sorted(os.listdir(test_dir), key=lambda x: int(os.path.splitext(x)[0])):\n",
    "        if not fname.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "            continue\n",
    "\n",
    "        img_path = os.path.join(test_dir, fname)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            class_logits, damage_logits = model(image)\n",
    "\n",
    "            class_pred_raw = class_logits.argmax(dim=1).item()\n",
    "            class_label = class_pred_raw + 18\n",
    "\n",
    "            damage_probs = torch.sigmoid(damage_logits).squeeze().cpu().numpy()\n",
    "            damage_pred = damage_probs > threshold\n",
    "\n",
    "            # === Filter only allowed damage indices for the predicted class\n",
    "            valid_indices = allowed_damage[class_label]\n",
    "\n",
    "            damage_labels = [str(i) for i in valid_indices if damage_pred[i]]\n",
    "\n",
    "            # 🔧 Fallback: assign top-1 if none passed threshold\n",
    "            if not damage_labels:\n",
    "                best_idx = valid_indices[np.argmax(damage_probs[valid_indices])]\n",
    "                damage_labels = [str(best_idx)]\n",
    "\n",
    "        # === Final formatted row\n",
    "        image_id = os.path.splitext(fname)[0]\n",
    "        full_row = [image_id, str(class_label)] + damage_labels\n",
    "        results.append(full_row)\n",
    "        label_counts.append(len(damage_labels))\n",
    "\n",
    "    # === Format and Save Submission CSV ===\n",
    "    df = pd.DataFrame(results)\n",
    "    df.columns = [\"ID\", \"class\"] + [f\"damage_{i}\" for i in range(df.shape[1] - 2)]\n",
    "\n",
    "    def clean_row(row):\n",
    "        return \",\".join([str(int(x)) for x in row if str(x).isdigit()])\n",
    "\n",
    "    df[\"class\"] = df.iloc[:, 1:].apply(clean_row, axis=1)\n",
    "    df = df[[\"ID\", \"class\"]]\n",
    "\n",
    "    filename = f\"submission_thresh_{str(threshold).replace('.', '')}.csv\"\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "    # === Logging ===\n",
    "    print(f\"\\n=== Threshold: {threshold} ===\")\n",
    "    print(f\"✅ Saved: {filename}\")\n",
    "    print(f\"Images: {len(label_counts)}\")\n",
    "    print(f\"Avg labels/image: {np.mean(label_counts):.2f}\")\n",
    "    print(f\"Min: {np.min(label_counts)}, Max: {np.max(label_counts)}, Median: {np.median(label_counts)}\")\n",
    "    print(\"Example rows:\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7328000,
     "sourceId": 11675883,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7462432,
     "sourceId": 11874251,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
